<!DOCTYPE html>
<html>
	<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Using Latent Dirichlet Allocation to Categorize My Twitter Feed</title>
	<meta name="viewport" content="width=device-width">
	<meta name="description" content="Bringing Your Ideas To Life">
	<link rel="canonical" href="/using-latent-dirichlet-allocation-to-categorize-my-twitter-feed">
	
	<link href='//fonts.googleapis.com/css?family=Titillium+Web:400,200|Lato:300,400,700|Pacifico' rel='stylesheet' type='text/css' />
	
	<!-- Custom CSS -->
	<link rel="stylesheet" href="/css/main.css">
	<link rel="stylesheet" href="/css/icons.css">
	
	<!-- start Mixpanel --><script type="text/javascript">(function(f,b){if(!b.__SV){var a,e,i,g;window.mixpanel=b;b._i=[];b.init=function(a,e,d){function f(b,h){var a=h.split(".");2==a.length&&(b=b[a[0]],h=a[1]);b[h]=function(){b.push([h].concat(Array.prototype.slice.call(arguments,0)))}}var c=b;"undefined"!==typeof d?c=b[d]=[]:d="mixpanel";c.people=c.people||[];c.toString=function(b){var a="mixpanel";"mixpanel"!==d&&(a+="."+d);b||(a+=" (stub)");return a};c.people.toString=function(){return c.toString(1)+".people (stub)"};i="disable track track_pageview track_links track_forms register register_once alias unregister identify name_tag set_config people.set people.set_once people.increment people.append people.track_charge people.clear_charges people.delete_user".split(" ");
	for(g=0;g<i.length;g++)f(c,i[g]);b._i.push([a,e,d])};b.__SV=1.2;a=f.createElement("script");a.type="text/javascript";a.async=!0;a.src="//cdn.mxpnl.com/libs/mixpanel-2.2.min.js";e=f.getElementsByTagName("script")[0];e.parentNode.insertBefore(a,e)}})(document,window.mixpanel||[]);
	mixpanel.init("55ef56f5e64295bfca50e477c0f0e0e2");</script><!-- end Mixpanel -->
	
	<script>
	mixpanel.track("LDA Article");
	</script>
	
</head>


	
	<body>
		<header class="site-header">
	<div class="wrap">
		<a class="site-title" href="/">
		</a>
		
		<nav class="site-nav">
			<div class="trigger">
				<a class="icons icon-info page-link" href="http://www.mathandpencil.com/capabilities"></a>
			</div>
		</nav>
	</div>
</header>
		
		<div class="page-content">
			<div class="wrap">
				<div class="post" style="padding:1.85em 0.75em;">
	<header class="post-header">
		<p class="meta">Jun 29, 2014</p>
		
		<h1>Using Latent Dirichlet Allocation to Categorize My Twitter Feed</h1>
	</header>
	
	<article class="post-content">
		<p>Over the past 3 years, I have tweeted about  4100 times, mostly URLS, and mostly about machine learning, statistics, big data, etc. I spent some time this past weekend seeing if I could categorize the tweets using Latent Dirichlet Allocation. For a great introduction to Latent Dirichlet Allocation (LDA), you can read the following link  <a href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/">here</a>. For the more mathematically inclined, you can read through this <a href="http://www.semanticsearchart.com/downloads/UWEETR-2010-0006.pdf">excellent paper</a> which explains LDA in a lot more detail.</p>

<p>The first step to categorizing my tweets was pulling the data. I initially downloaded and installed Twython and tried to pull all of my tweets using the Twitter API, but that quickly realized there was an archive button under settings. So I stopped writing code and just double clicked the archive button. Apparently 4100 tweets is fairly easy to archive, because I received an email from Twitter within 15 seconds with a download link.</p>

<p>After downloading my Twitter archive, I opened up tweets.csv (provided by twitter) and extracted the single column containing the tweets to a new file for additional processing. I did this using unix awk:</p>

<p><code>
cat tweets.csv | awk -F"," '{print $6}' &gt; tweets_content.dat
</code></p>

<p>Next, I needed to extract the URLs out of the tweets (I do not care about the content for this type of analysis). To extract the URLs, I wrote a simple Python script:</p>

<div class="highlight"><pre><code class="python"><span class="c">#!/usr/bin/env python</span>
<span class="c"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">readlines</span><span class="p">():</span>
	<span class="n">results</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s">&quot;(?P&lt;url&gt;https?://[^\s]+)&quot;</span><span class="p">,</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">())</span>
	<span class="k">if</span> <span class="n">results</span><span class="p">:</span>
		<span class="k">print</span> <span class="n">results</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="s">&quot;url&quot;</span><span class="p">)</span></code></pre></div>

<p>After running this script across tweets_content.dat, I am left with a file what contains only the URLS I tweeted out. Here is a <a href="https://gist.github.com/josephmisiti/782f8433b393069dfe4c">gist</a> of all of the URLS I found. </p>

<p>Once I had the URLS, I needed to be able to extract the information in the URLS to build a corpus for LDA analysis. Unfortunately, this was not as easy as I thought it was going to be. I initially started using Python’s excellent NLTK library to extract the HTML using the following command:</p>

<div class="highlight"><pre><code class="python"><span class="n">nltk</span><span class="o">.</span><span class="n">clean_html</span><span class="p">(</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span></code></pre></div>

<p>After spot checking the results, I realized that this was not successfully retrieving HTML for about 1/3 of my URLS.  I did some more research and found <a href="https://www.diffbot.com/">DiffBot</a>, which uses machine learning to figure out what content is valid and removes ads, etc. They will give you 10K calls/day for free, so I used their <a href="http://diffbot.com/dev/docs/bulk/">bulk API</a> to retrieve the content of my URLS (parsed HTML). The bulk API call looks something like this.</p>

<div class="highlight"><pre><code class="python"><span class="c">#!/usr/bin/env python</span>
<span class="c"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="n">DIFF_BOT_TOKEN</span> <span class="o">=</span> <span class="s">&#39;xxxxxxxx&#39;</span>

<span class="k">def</span> <span class="nf">create_bulk_job</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
	<span class="n">apiUrl</span> <span class="o">=</span> <span class="s">&#39;http://api.diffbot.com/v3/article&#39;</span>
	<span class="n">params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">DIFF_BOT_TOKEN</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">urls</span><span class="o">=</span><span class="n">urls</span><span class="p">,</span> <span class="n">apiUrl</span><span class="o">=</span><span class="n">apiUrl</span><span class="p">)</span>
	<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="s">&#39;http://api.diffbot.com/v3/bulk&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
	
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&#39;__main__&#39;</span><span class="p">:</span>
	<span class="n">urls</span> <span class="o">=</span> <span class="s">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">u</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s">&#39;&quot;&#39;</span><span class="p">,</span><span class="s">&#39;&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">readlines</span><span class="p">()])</span>
	<span class="n">name</span> <span class="o">=</span> <span class="s">&#39;MISITI&#39;</span>
	<span class="n">create_bulk_job</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls</span><span class="p">,</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span></code></pre></div>

<p>After about 10 minutes, DiffBot was able to crawl all of the URLs I sent it, and I downloaded the results into JSON format. Now that I had the html from each of the URLs, I used the diffbot html to make my corpus. I didn’t use any sophisticated techniques to create the corpus (although I think you could add some here to see better performance) - I wrote a quick script that extracted only words, made them lower case, and then remove all English stop words using NLTK.</p>

<div class="highlight"><pre><code class="python"><span class="c">#!/usr/bin/env python</span>
<span class="c"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">nltk</span>

<span class="n">STOP_WORDS</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">corpus</span><span class="o">.</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s">&#39;english&#39;</span><span class="p">)</span>
<span class="n">CORPUS_FILE</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;corpus.txt&quot;</span><span class="p">,</span> <span class="s">&quot;w&quot;</span><span class="p">)</span>

<span class="n">tweets</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">&quot;results.json&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets</span><span class="p">:</span>
	<span class="n">html</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&quot;text&quot;</span><span class="p">,</span><span class="s">&quot;&quot;</span><span class="p">)</span>
	<span class="n">words</span> <span class="o">=</span> <span class="n">html</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
	<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="s">&#39;^[\w-]+$&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">]</span> <span class="c"># take only alphanumerics</span>
	<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="p">]</span>
	<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">STOP_WORDS</span><span class="p">]</span>
	
	<span class="n">CORPUS_FILE</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">+</span> <span class="s">&quot;</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">)</span>
	
<span class="n">CORPUS_FILE</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></code></pre></div>

<p>After the corpus was created, I was able to run LDA on the data set. The inputs to the LDA process are the number of topics K, the corpus file (corpus.txt in this case), and priors for the alpha/beta distributions. I used default priors and ran LDA for 50 iterations. The whole process took about 10 minutes and my results can be seen in 10 gists below. I searched K from 5 - 20 and 10 topics yielded the best results. I did not take into account perplexity or any other metrics for making my decision. A spot check of the top words in each topic yielded what appeared to be the following categories.</p>

<p><a href="https://gist.github.com/josephmisiti/3ed68c02d3d6877a3097">Topic 1</a> - social media / tech companies</p>

<p><a href="https://gist.github.com/josephmisiti/2a7a6f58f0b4e1dbdb43">Topic 2</a> - generic</p>

<p><a href="https://gist.github.com/josephmisiti/daebfd66d913dc426ec8">Topic 3</a> - software / cloud computing / search</p>

<p><a href="https://gist.github.com/josephmisiti/45b31c52167af8c37735">Topic 4</a> - cities</p>

<p><a href="https://gist.github.com/josephmisiti/061042ecd684b1831e32">Topic 5</a> - government / nsa  / intelligence</p>

<p><a href="https://gist.github.com/josephmisiti/b4a8e97dd2cc09d0d367">Topic 6</a> - banking / finance / economics</p>

<p><a href="https://gist.github.com/josephmisiti/037141fdf95c887cf6a0">Topic 7</a> - cloud computing / ec2 / the big lebowski</p>

<p><a href="https://gist.github.com/josephmisiti/dc94723022d4e5630f7c">Topic 8</a> - world events</p>

<p><a href="https://gist.github.com/josephmisiti/8be39c0b6fd4a353a80e">Topic 9</a> - machine learning / big data / statistics</p>

<p><a href="https://gist.github.com/josephmisiti/50915dda4f745c572905">Topic 10</a> - religion / sexuality</p>

<p>The final step to the process was calculating the topic distriubtion of each url/document. This can be done by simply summing the topic distribution vectors that correspond to each word in the document. These vectors are one of the outputs of the LDA process. After the vector sums are calculated, you normalize each vector (because they are, after all, a probability mass function). I used the following code to calculate the distributions of each url.</p>

<div class="highlight"><pre><code class="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cPickle</span> <span class="kn">as</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">json</span>

<span class="p">(</span><span class="n">ldak</span><span class="p">,</span> <span class="n">phi</span><span class="p">,</span> <span class="n">voca</span><span class="p">)</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">&#39;ldaphi_K10.p&#39;</span><span class="p">,</span> <span class="s">&quot;rb&quot;</span><span class="p">))</span>

<span class="n">words_to_column</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">voca</span><span class="p">):</span>
	<span class="n">words_to_column</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span>
		
<span class="n">tweets</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">&quot;results.json&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="k">for</span> <span class="n">tweet</span> <span class="ow">in</span> <span class="n">tweets</span><span class="p">:</span>
	<span class="n">html</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;text&#39;</span><span class="p">)</span>
	<span class="n">title</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;title&#39;</span><span class="p">,</span><span class="s">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s">&quot;ascii&quot;</span><span class="p">,</span><span class="s">&quot;ignore&quot;</span><span class="p">)</span>
	<span class="n">url</span> <span class="o">=</span> <span class="n">tweet</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">&#39;resolvedPageUrl&#39;</span><span class="p">,</span><span class="s">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s">&quot;ascii&quot;</span><span class="p">,</span><span class="s">&quot;ignore&quot;</span><span class="p">)</span>
	
	<span class="n">sum_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">ldak</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">html</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
		<span class="n">word</span> <span class="o">=</span> <span class="n">word</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
		<span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words_to_column</span><span class="p">:</span>
			<span class="n">sum_vector</span> <span class="o">+=</span> <span class="n">phi</span><span class="p">[:,</span> <span class="n">words_to_column</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span>
	<span class="k">if</span> <span class="n">sum_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> 
		<span class="n">sum_vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ldak</span><span class="p">)</span><span class="o">/</span><span class="n">ldak</span><span class="p">;</span>
	<span class="k">else</span><span class="p">:</span> 
		<span class="n">sum_vector</span> <span class="o">=</span> <span class="n">sum_vector</span> <span class="o">/</span> <span class="n">sum_vector</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
		
	<span class="k">print</span> <span class="s">&quot;</span><span class="si">%s</span><span class="s">,</span><span class="si">%s</span><span class="s">,</span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="s">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="s">&quot;</span><span class="si">%2.3f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sum_vector</span><span class="p">]),</span><span class="n">title</span><span class="p">,</span><span class="n">url</span><span class="p">)</span></code></pre></div>

<p>I uploaded the results in CSV format to Google Drive and anyone can download the data <a href="https://docs.google.com/spreadsheets/d/1iwvCYeEEOF-4BtLf7gKUAA638fyftdlcgETpZeRYcP8/edit?usp=sharing">here</a>. You can sort A-Z on any of the 10 topic columns and theoretically the appropriate URLS should float to the top. This process is not perfect, but I was pretty impressed with the results. I think if I tweeted a lot more, the number of unique words in my corpus (the vocabulary) would be much larger and would probably lead to better segmentation. In a future blog post, I plan on taking perplexity and the priors into account also.</p>

<p>If you are interested in machine learning, math, and statistics you can following me on twitter: <a href="https://www.twitter.com/josephmisiti">@josephmisiti</a> - I spend all day tweeting about it.</p>

<p>If you need help with your data set, contact my firm at info@mathandpencil.com</p>

<p>You can clone this <a href="https://github.com/mathandpencil/twitter-lda">repo</a>, but just a warning, I did not take anytime to organize the code or the results. </p>

	</article>
</div>
			</div>
		</div>
		<footer class="site-footer">
	<div class="wrap">
		<div class="footer-col-1 column">
			<ul>
				<li></li>
				<li>
					<a href="mailto:info@mathandpencil.com">info@mathandpencil.com</a>
				</li>
			</ul>
		</div>
		
		<div class="footer-col-2 column">
			<ul>
				<li>
					<a class="icons icon-signup" href="/feed.xml"></a>
				</li>
				
				<li>
					<a class="icons icon-github2" href="https://github.com/mathandpencil">
					</a>
				</li>
				
				<li>
					<a class="icons icon-twitter" href="https://twitter.com/mathandpencil">
					</a>
				</li>
			</ul>
		</div>
		
		<div class="footer-col-3 column">
			<p class="text">Bringing Your Ideas To Life</p>
		</div>
	</div>
</footer>
	</body>
</html>